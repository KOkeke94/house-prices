---
title: "model"
author: "ddxbugs"
date: "2024-05-27"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
sessionInfo()
getwd()

set.seed(1234)

if (!require(caret)) install.packages("caret")
if (!require(caret)) install.packages("lmboot")
if (!require(caret)) install.packages("glmnet")
if (!require(caret)) install.packages("olsrr")
if (!require(caret)) install.packages("car")
if (!require(caret)) install.packages("boot")
library(caret)
library(glmnet)
library(olsrr)
library(car)
library(lmboot)
library(boot)
```

## Import the data
```{r}
processed_data <- read.csv("../data/processed/processed_data.csv")
```

## Transform the data
```{r}
# Factorize variable data types
processed_data$LandContour <- factor(processed_data$LandContour)
processed_data$LotConfig <- factor(processed_data$LotConfig)
processed_data$Neighborhood <- factor(processed_data$Neighborhood)
processed_data$BldgType <- factor(processed_data$BldgType)
#processed_data$OverallQual <- factor(processed_data$OverallQual)
#processed_data$OverallCond <- factor(processed_data$OverallCond)
#processed_data$YearBuilt <- factor(processed_data$YearBuilt)
processed_data$CentralAir <- factor(processed_data$CentralAir)
processed_data$PavedDrive <- factor(processed_data$PavedDrive)

# Store distinct data type values
land_contour <- processed_data %>% distinct(LandContour) %>% pull(LandContour)
lot_config <- processed_data %>% distinct(LotConfig) %>% pull(LotConfig)
neighborhood <- processed_data %>% distinct(Neighborhood) %>% pull(Neighborhood)
bldg_type <- processed_data %>% distinct(BldgType) %>% pull(BldgType)
overall_qual <- processed_data %>% distinct(OverallQual) %>% pull(OverallQual)
overall_cond <- processed_data %>% distinct(OverallCond) %>% pull(OverallCond)
year_built <- processed_data %>% distinct(YearBuilt) %>% pull(YearBuilt)
central_air <- processed_data %>% distinct(CentralAir) %>% pull(CentralAir)
tot_rms_abv_grd <- processed_data %>% distinct(TotRmsAbvGrd) %>% pull(TotRmsAbvGrd)
fireplaces <- processed_data %>% distinct(Fireplaces) %>% pull(Fireplaces)
paved_drive <- processed_data %>% distinct(PavedDrive) %>% pull(PavedDrive)

transformed_data <- processed_data %>% select(-X)
```

## Build a model
```{r}
fit <- lm(SalePrice~., data=transformed_data)
summary(fit)
anova(fit)
confint(fit)
plot(fit)
hist(fit$residuals, main="Histogram of Residuals", xlab="Residuals")
vif(fit)
barplot(vif(fit), main="VIF Values", horiz=TRUE, col="lightblue")
```

## Feature Selection
```{r}
ols_step_backward_p(fit, prem=0.05, details=FALSE)
ols_step_forward_p(fit, penter=0.05, details=FALSE)
ols_step_both_p(fit, prem=0.05, penter=0.05, details=TRUE)
```

## Stepwise selection MLR model
```{r}
stepwise_fit <- lm(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data)
summary(stepwise_fit)
anova(stepwise_fit)
confint(stepwise_fit)
plot(stepwise_fit)
hist(stepwise_fit$residuals, breaks="Sturges", main="Histogram of Residuals", xlab="Residuals")
vif(stepwise_fit)
barplot(vif(fit), main="VIF Values", horiz=TRUE, col="lightblue")
```

## OBJECTIVE 1
```{r}

# Bootstrapping 
# Bootstrap 95% CI
# Bagging with Linear Regression

train_control <- trainControl(method="cv", number=5)
model <- train(log(SalePrice)~., data=transformed_data, method="lm", trControl=train_control)
summary(model)

stepwise_model <- train(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data, method="lm", trControl=train_control)
summary(stepwise_model)

```

## OBJECTIVE 2

# GLMNET
```{r}
# Set k-fold parameters
glm_Control <- trainControl(method="repeatedcv", number=5, repeats=1)
# Fit GLMNET
glmnet.fit <- train(log(SalePrice)~., data=transformed_data, method="glmnet", trControl=glm_Control)
glmnet.fit
plot(glmnet.fit)

# Investigate penalty term coefficients
opt.pen <- glmnet.fit$finalModel$lambdaOpt
coef(glmnet.fit$finalModel, opt.pen)
```
# Bootstrapping
```{r}
# Boot function calculates median Sale Price
median.func <- function(x, i) {
  median(x[i])
}
median.func(transformed_data$SalePrice, 1:30)
median(transformed_data$SalePrice)

median.boot <- boot(transformed_data$SalePrice, median.func, R=2000)
median.boot

# Obtain bootstrap statistical CI
boot.ci(median.boot, conf=0.95, type=c("perc", "basic", "bca"))

# Take a sample of 30 observations to estimate median of population
boot.sample <- sample(transformed_data$SalePrice, 30, replace=FALSE)
hist(boot.sample)
median(boot.sample)
# Compute the bootstrap sampling distribution of the median
medians <- c()
B <- 2000
for (i in 1:B) {
  b.sample <- sample(processed_data$SalePrice, 30, replace=TRUE)
  medians[i] <- median(b.sample)
}
# Plot sample Sale Price histogram distribution
hist(medians, main="Bootstrap Sampling Distribution", xlab="Median Sale Price ($ USD)")

# Obtain SE of median
sd(medians)
# 95% confidence interval
quantile(medians, probs=c(0.025,0.975))

boot.p <- paired.boot(log(SalePrice)~., data=transformed_data, B=2000, seed=1234)
# Obtain bootstrap percentile CI for each coefficient
apply(boot.p$bootEstParam, 2, quantile, probs=c(0.025, 0.975))

boot.res <- residual.boot(log(SalePrice)~., data=transformed_data, B=2000, seed=1234)
apply(boot.res$bootEstParam, 2, quantile, probs=c(0.025, 0.975))
```

## Compare Bootstrapping vs. T-test result 
```{r}
t.result <- t.test(transformed_data$SalePrice)
xbar <- c()
B <- 2000
for (i in 1:B) {
  b.sample <- sample(transformed_data$SalePrice, 30, replace=TRUE)
  xbar[i] <- mean(b.sample)
}
# Plot xbar Sale Price histogram distribution
hist(xbar, main="Bootstrap Sampling Distribution", xlab="Average Sale Price")
# Obtain SE of the xbar
sd(xbar)
# 95% Confidence interval for population mean
quantile(xbar, probs=c(0.025, 0.975))
t.result$conf.int
# T-test SE of mean
sd(transformed_data$SalePrice)/sqrt(30)
# Bootstrap SE for the mean
sd(xbar)
```

## K-fold Cross Validation 
```{r}
# Quick fit K=1 and K=30
predictors <- data.frame(
  #LandContour=land_contour,
  #LotConfig=lot_config,
  #Neighborhood=neighborhood,
  #BldgType=bldg_type,
  #OverallQual=overall_qual,
  #OverallCond=overall_cond,
  #OverallQual=seq(min(transformed_data$OverallQual), max(transformed_data$OverallQual), length.out = 50),
  #OverallCond=seq(min(transformed_data$OverallCond), max(transformed_data$OverallCond), length.out = 50),
  #YearBuilt=seq(min(transformed_data$YearBuilt), max(transformed_data$YearBuilt), length.out = 50),
  #CentralAir=central_air,
  #X1stFlrSF=seq(min(transformed_data$X1stFlrSF), max(transformed_data$X1stFlrSF), length.out = 50)
  #X2ndFlrSF=seq(min(transformed_data$X2ndFlrSF), max(transformed_data$X2ndFlrSF), length.out = 50),
  #TotRmsAbvGrd=tot_rms_abv_grd,
  #TotRmsAbvGrd=seq(min(transformed_data$TotRmsAbvGrd), max(transformed_data$TotRmsAbvGrd), length.out = 50),
  #Fireplaces=fireplaces,
  #PavedDrive=paved_drive
)

# Set K-fold parameters
knn_Control <- trainControl(method="repeatedcv", number=10, repeats=1)
knn.fit <- train(log(SalePrice)~., 
                 data=transformed_data, 
                 method="knn", 
                 trControl=knn_Control, 
                 tuneGrid=expand.grid(k=c(1:10, 20, 30)))
plot(knn.fit)
knn.fit

predictions <- predict(knn.fit, predictors)
prediction.surface <- matrix(predictions, 301, 51)
surface3d(0:300, 0:50, prediction.surface, alpha=0.4)
```

```{r}
pred <- predict(knn.fit, predictors)
pred.surface <- matrix(preds, 301, 51)
plot3d()
surface3d()
```



```{r}
# Random Forest Bootstrapping

# Random Forest Bagging
library(randomForest)
library(ggplot2)

# Set the simulated data seed
set.seed(1234)

```

## Bagging with Multiple Linear Regression
```{r}
# Load necessary libraries
library(caret)
library(ipred)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Create a formula
formula <- y ~ poly(x, 4, raw=TRUE)

# Perform Bagging
set.seed(123)
bagging_model <- bagging(formula, data = data, nbagg = 50)

# Predict and plot
preds <- predict(bagging_model, data)
plot(x, y, main = "Bagging with Linear Regression")
lines(x, preds, col = "red", lwd = 2)

```


## Bootstrapping and Bagging with Random Forest
```{r}
# Load necessary libraries
library(randomForest)
library(ggplot2)

# Fit Random Forest
set.seed(123)
rf_model <- randomForest(SalePrice ~ ., data = processed_data, ntree = 500)

# Predict and plot
preds <- predict(rf_model, data)

# Plot original data and predictions
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = preds), color = "blue", size = 1.5) +
  ggtitle("Random Forest with Bootstrapping and Bagging") +
  theme_minimal()

```


## Bootstrapping with Random Forest Manually
```{r}
# Load necessary libraries
library(caret)
library(rpart)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Number of bootstraps
num_bootstrap <- 50

# Store predictions
pred_matrix <- matrix(NA, nrow = num_bootstrap, ncol = length(y))

# Perform bootstrapping
set.seed(123)
for (i in 1:num_bootstrap) {
  # Create bootstrap sample
  boot_indices <- sample(1:nrow(data), replace = TRUE)
  boot_data <- data[boot_indices, ]
  
  # Train decision tree on bootstrap sample
  tree_model <- rpart(y ~ x, data = boot_data)
  
  # Predict on original data
  pred_matrix[i, ] <- predict(tree_model, newdata = data)
}

# Aggregate predictions (bagging)
bagged_preds <- apply(pred_matrix, 2, mean)

# Plot original data and bagged predictions
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = bagged_preds), color = "red", size = 1.5) +
  ggtitle("Bagging with Decision Trees") +
  theme_minimal()
```


## Ensemble multiple models
```{r}
# Load necessary libraries
library(caret)
library(glmnet)
library(randomForest)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Define training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")

# List of models to ensemble
model_list <- caretList(
  y ~ poly(x, 4, raw=TRUE),
  data = data,
  trControl = train_control,
  methodList = c("lm", "glmnet", "rf")
)

# Stacking models using linear regression as meta-model
stack_control <- trainControl(method = "cv", number = 5)
stacked_model <- caretStack(
  model_list,
  method = "glm",
  trControl = stack_control
)

# Predict and plot
preds <- predict(stacked_model, newdata = data)
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = preds), color = "green", size = 1.5) +
  ggtitle("Ensemble (Stacking)") +
  theme_minimal()

```


