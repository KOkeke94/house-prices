---
title: "model"
author: "ddxbugs"
date: "2024-05-27"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
sessionInfo()
getwd()

set.seed(1234)

if (!require(caret)) install.packages("caret")
if (!require(caret)) install.packages("lmboot")
if (!require(caret)) install.packages("glmnet")
if (!require(caret)) install.packages("olsrr")
if (!require(caret)) install.packages("car")
library(caret)
library(glmnet)
library(olsrr)
library(car)
library(lmboot)
```

## Import the data
```{r}
processed_data <- read.csv("../data/processed/processed_data.csv")
```

## Transform the data
```{r}
transformed_data <- processed_data %>%
  mutate(SalePrice=log(SalePrice))
```

## Feature Selection
```{r}
ols_step_backward_p(fit, prem=0.05, details=FALSE)
ols_step_forward_p(fit, penter=0.05, details=FALSE)
ols_step_both_p(fit, prem=0.05, penter=0.05, details=TRUE)
```

## OBJECTIVE 1
```{r}

# Bootstrapping 
# Bootstrap 95% CI
# Bagging with Linear Regression

train_control <- trainControl(method="cv", number=5)
model <- train(SalePrice ~ ., data=transformed_data, method="lm", trControl=train_control)
summary(model)

stepwise_model <- train(SalePrice ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data, method="lm", trControl=train_control)
summary(stepwise_model)

```

## Fit a model
```{r}
fit <- lm(SalePrice~., data=transformed_data)
summary(fit)
anova(fit)
confint(fit)
plot(fit)
hist(fit$residuals, main="Histogram of Residuals", xlab="Residuals")
vif(fit)
barplot(vif(fit), main="VIF Values", horiz=TRUE, col="lightblue")
```

```{r}
stepwise_fit <- lm(SalePrice ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data)
summary(stepwise_fit)
anova(stepwise_fit)
plot(stepwise_fit)
hist(stepwise_fit$residuals, breaks="Sturges", main="Histogram of Residuals", xlab="Residuals")
vif(stepwise_fit)
barplot(vif(fit), main="VIF Values", horiz=TRUE, col="lightblue")
```





## OBJECTIVE 2

# Bootstrapping
```{r}

# Take a sample of 30 observations to estimate median of population
boot.sample <- sample(transformed_data, 100, replace=TRUE)
hist(boot.sample$SalePrice)
median(boot.sample$SalePrice)
# Compute the bootstrap sampling distribution of the median
medians <- c()
B <- 2000
for (i in 1:B) {
  b.sample <- sample(preprocessed_data$SalePrice, 30, replace=TRUE)
  medians[i] <- median(b.sample)
}
hist(medians, main="Bootstrap Sampling Distribution", xlab="Median Sale Price ($ USD)")

# Obtain SE of median 
sd(medians)

boot.p <- paired.boot(SalePrice~., data=transformed_data, B=2000, seed=1234)
# Obtain bootstrap percentile CI for each coefficient
apply(boot.p$bootEstParam, 2, quantile, probs=c(0.025, 0.975))

boot.res <- residual.boot(SalePrice~., data=transformed_data, B=2000, seed=1234)
apply(boot.res$bootEstParam, 2, quantile, probs=c(0.025, 0.975))
```

```{r}
knnControl <- trainControl(method="repeatedcv", number=10, repeats=1)
knn.fit <- train(SalePrice~., data=processed_data, method="knn", trControl=knnControl)
plot(knn.fit)
knn.fit
```
```{r}
pred <- predict(knn.fit, predictors)
pred.surface <- matrix(preds, 301, 51)
plot3d()
surface3d()
```



```{r}
# Random Forest Bootstrapping

# Random Forest Bagging
library(randomForest)
library(ggplot2)

# Set the simulated data seed
set.seed(1234)

```

## Bagging with Multiple Linear Regression
```{r}
# Load necessary libraries
library(caret)
library(ipred)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Create a formula
formula <- y ~ poly(x, 4, raw=TRUE)

# Perform Bagging
set.seed(123)
bagging_model <- bagging(formula, data = data, nbagg = 50)

# Predict and plot
preds <- predict(bagging_model, data)
plot(x, y, main = "Bagging with Linear Regression")
lines(x, preds, col = "red", lwd = 2)

```


## Bootstrapping and Bagging with Random Forest
```{r}
# Load necessary libraries
library(randomForest)
library(ggplot2)

# Fit Random Forest
set.seed(123)
rf_model <- randomForest(SalePrice ~ ., data = processed_data, ntree = 500)

# Predict and plot
preds <- predict(rf_model, data)

# Plot original data and predictions
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = preds), color = "blue", size = 1.5) +
  ggtitle("Random Forest with Bootstrapping and Bagging") +
  theme_minimal()

```


## Bootstrapping with Random Forest Manually
```{r}
# Load necessary libraries
library(caret)
library(rpart)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Number of bootstraps
num_bootstrap <- 50

# Store predictions
pred_matrix <- matrix(NA, nrow = num_bootstrap, ncol = length(y))

# Perform bootstrapping
set.seed(123)
for (i in 1:num_bootstrap) {
  # Create bootstrap sample
  boot_indices <- sample(1:nrow(data), replace = TRUE)
  boot_data <- data[boot_indices, ]
  
  # Train decision tree on bootstrap sample
  tree_model <- rpart(y ~ x, data = boot_data)
  
  # Predict on original data
  pred_matrix[i, ] <- predict(tree_model, newdata = data)
}

# Aggregate predictions (bagging)
bagged_preds <- apply(pred_matrix, 2, mean)

# Plot original data and bagged predictions
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = bagged_preds), color = "red", size = 1.5) +
  ggtitle("Bagging with Decision Trees") +
  theme_minimal()
```


## Ensemble multiple models
```{r}
# Load necessary libraries
library(caret)
library(glmnet)
library(randomForest)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Define training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")

# List of models to ensemble
model_list <- caretList(
  y ~ poly(x, 4, raw=TRUE),
  data = data,
  trControl = train_control,
  methodList = c("lm", "glmnet", "rf")
)

# Stacking models using linear regression as meta-model
stack_control <- trainControl(method = "cv", number = 5)
stacked_model <- caretStack(
  model_list,
  method = "glm",
  trControl = stack_control
)

# Predict and plot
preds <- predict(stacked_model, newdata = data)
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = preds), color = "green", size = 1.5) +
  ggtitle("Ensemble (Stacking)") +
  theme_minimal()

```


