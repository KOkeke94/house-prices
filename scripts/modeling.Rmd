---
title: "model"
author: "ddxbugs"
date: "2024-05-27"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
sessionInfo()
getwd()

set.seed(1234)

if (!require(caret)) install.packages("caret")
if (!require(lmboot)) install.packages("lmboot")
if (!require(glmnet)) install.packages("glmnet")
if (!require(olsrr)) install.packages("olsrr")
if (!require(car)) install.packages("car")
if (!require(boot)) install.packages("boot")
if (!require(gridExtra)) install.packages("gridExtra")
if (!require(scales)) install.packages("scales")
if (!require(ipred)) install.packages("ipred")
if (!require(randomForest)) install.packages("randomForest")
if (!require(caretEnsemble)) install.packages("caretEnsemble")
library(caret)
library(glmnet)
library(olsrr)
library(car)
library(lmboot)
library(boot)
library(gridExtra)
library(scales)
library(ipred)
library(randomForest)
library(caretEnsemble)
```

## Import the data
```{r}
processed_data <- read.csv("../data/processed/processed_data.csv")
```

## Transform the data
```{r}
# Factorize variable data types
processed_data$LandContour <- factor(processed_data$LandContour)
processed_data$LotConfig <- factor(processed_data$LotConfig)
processed_data$Neighborhood <- factor(processed_data$Neighborhood)
processed_data$BldgType <- factor(processed_data$BldgType)
#processed_data$OverallQual <- factor(processed_data$OverallQual)
#processed_data$OverallCond <- factor(processed_data$OverallCond)
#processed_data$YearBuilt <- factor(processed_data$YearBuilt)
processed_data$CentralAir <- factor(processed_data$CentralAir)
processed_data$PavedDrive <- factor(processed_data$PavedDrive)

# Store distinct data type values
land_contour <- processed_data %>% distinct(LandContour) %>% pull(LandContour)
lot_config <- processed_data %>% distinct(LotConfig) %>% pull(LotConfig)
neighborhood <- processed_data %>% distinct(Neighborhood) %>% pull(Neighborhood)
bldg_type <- processed_data %>% distinct(BldgType) %>% pull(BldgType)
overall_qual <- processed_data %>% distinct(OverallQual) %>% pull(OverallQual)
overall_cond <- processed_data %>% distinct(OverallCond) %>% pull(OverallCond)
year_built <- processed_data %>% distinct(YearBuilt) %>% pull(YearBuilt)
central_air <- processed_data %>% distinct(CentralAir) %>% pull(CentralAir)
tot_rms_abv_grd <- processed_data %>% distinct(TotRmsAbvGrd) %>% pull(TotRmsAbvGrd)
fireplaces <- processed_data %>% distinct(Fireplaces) %>% pull(Fireplaces)
paved_drive <- processed_data %>% distinct(PavedDrive) %>% pull(PavedDrive)

# Create transformed data set 
transformed_data <- processed_data %>% select(-X)

# Define the list of predictors
predictors <- c("LandContour", "LotConfig", "Neighborhood", "BldgType", 
                "OverallQual", "OverallCond", "YearBuilt", "CentralAir", 
                "X1stFlrSF", "X2ndFlrSF", "TotRmsAbvGrd", "Fireplaces", "PavedDrive")
```

## Build a model
```{r}
#fit <- lm(SalePrice~., data=transformed_data)
#fit <- lm(log(SalePrice)~., data=transformed_data)
fit <- lm(log(SalePrice)~., data=transformed_data)
summary(fit)
anova(fit)
confint(fit)
plot(fit)
hist(fit$residuals, main="Histogram of Residuals", xlab="Residuals")
vif(fit)
barplot(vif(fit), main="VIF Values", horiz=TRUE, col="lightblue")
```

## Feature Selection
```{r}
ols_step_backward_p(fit, prem=0.05, details=FALSE)
ols_step_forward_p(fit, penter=0.05, details=FALSE)
ols_step_both_p(fit, prem=0.05, penter=0.05, details=TRUE)
```

## Stepwise selection MLR model
```{r}
stepwise_fit <- lm(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data)
summary(stepwise_fit)
anova(stepwise_fit)
confint(stepwise_fit)
plot(stepwise_fit)
hist(stepwise_fit$residuals, breaks="Sturges", main="Histogram of Residuals", xlab="Residuals")
vif(stepwise_fit)
barplot(vif(stepwise_fit), main="VIF Values", horiz=TRUE, col="lightblue")
```

## Custom model
```{r}
reduced_fit <- lm(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + BldgType + Neighborhood,
                  data=transformed_data)
summary(reduced_fit)
anova(reduced_fit)
confint(reduced_fit)
plot(reduced_fit)
hist(reduced_fit$residuals, breaks="Sturges", main="Histogram of Residuals", xlab="Residuals")
vif(reduced_fit)
barplot(vif(reduced_fit), main="VIF Values", horiz=TRUE, col="lightblue")
```


## OBJECTIVE 1
```{r}
# Bootstrapping linear regression model
lmControl <- trainControl(method="cv", number=5)

full_model <- train(log(SalePrice)~., data=transformed_data, method="lm", trControl=lmControl)
summary(full_model)

stepwise_model <- train(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data, method="lm", trControl=lmControl)
summary(stepwise_model)

custom_model <- train(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + BldgType + Neighborhood,
                  data=transformed_data, method="lm", trControl=lmControl)
summary(custom_model)
```

## OBJECTIVE 2

# GLMNET
```{r}
# Set k-fold parameters
glmControl <- trainControl(method="repeatedcv", number=5, repeats=1)
# Fit GLMNET
glmnet.fit <- train(log(SalePrice)~., data=transformed_data, method="glmnet", trControl=glmControl)
glmnet.fit
plot(glmnet.fit)

plot(glmnet.fit$finalModel, xvar="lambda", label=TRUE)

# Investigate penalty term coefficients
opt.pen <- glmnet.fit$finalModel$lambdaOpt
coef(glmnet.fit$finalModel, opt.pen)

# Create dummy variable for categorical predictors
dummy_data <- model.matrix(SalePrice~.-1, data=transformed_data)

# Split data into predictor (x) and response (y)
x <- dummy_data
y <- transformed_data$SalePrice
grid <- 10^seq(10, -2, length=100)

# LASSO
lasso.mod <- glmnet(x, y, alpha=1, lambda=grid)
cv.out <- cv.glmnet(x, y, alpha=1)
plot(cv.out)
best_lambda <- cv.out$lamdba.1se
coef(cv.out, s=best_lambda)

# RIDGE
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)
cv.out <- cv.glmnet(x, y, alpha=0)
plot(cv.out)
best_lambda <- cv.out$lambda.1se
coef(cv.out, s=best_lambda)

```
# Bootstrapping descriptive statistic
```{r}
# Boot function calculates median Sale Price
median.func <- function(x, i) {
  median(x[i])
}
median.func(transformed_data$SalePrice, 1:30)
median(transformed_data$SalePrice)

median.boot <- boot(transformed_data$SalePrice, median.func, R=2000)
median.boot

# Obtain bootstrap statistical CI
boot.ci(median.boot, conf=0.95, type=c("perc", "basic", "bca"))

# Take a sample of 30 observations to estimate median of population
boot.sample <- sample(transformed_data$SalePrice, 30, replace=FALSE)
hist(boot.sample)
median(boot.sample)
# Compute the bootstrap sampling distribution of the median
medians <- c()
B <- 2000
for (i in 1:B) {
  b.sample <- sample(processed_data$SalePrice, 30, replace=TRUE)
  medians[i] <- median(b.sample)
}
# Plot sample Sale Price histogram distribution
hist(medians, main="Bootstrap Sampling Distribution", xlab="Median Sale Price ($ USD)")

# Obtain SE of median
sd(medians)
# 95% confidence interval
quantile(medians, probs=c(0.025,0.975))

boot.p <- paired.boot(log(SalePrice)~., data=transformed_data, B=2000, seed=1234)
# Obtain bootstrap percentile CI for each coefficient
apply(boot.p$bootEstParam, 2, quantile, probs=c(0.025, 0.975))

boot.res <- residual.boot(log(SalePrice)~., data=transformed_data, B=2000, seed=1234)
apply(boot.res$bootEstParam, 2, quantile, probs=c(0.025, 0.975))
```

## Compare Bootstrapping vs. T-test result 
```{r}
t.result <- t.test(transformed_data$SalePrice)
xbar <- c()
B <- 2000
for (i in 1:B) {
  b.sample <- sample(transformed_data$SalePrice, 30, replace=TRUE)
  xbar[i] <- mean(b.sample)
}
# Plot xbar Sale Price histogram distribution
hist(xbar, main="Bootstrap Sampling Distribution", xlab="Average Sale Price")
# Obtain SE of the xbar
sd(xbar)
# 95% Confidence interval for population mean
quantile(xbar, probs=c(0.025, 0.975))
t.result$conf.int
# T-test SE of mean
sd(transformed_data$SalePrice)/sqrt(30)
# Bootstrap SE for the mean
sd(xbar)
```

## K-fold Cross Validation 
```{r}
# Quick fit K=1 and K=30
#predictors <- data.frame(
  #LandContour=land_contour,
  #LotConfig=lot_config,
  #Neighborhood=neighborhood,
  #BldgType=bldg_type,
  #OverallQual=overall_qual,
  #OverallCond=overall_cond,
  #OverallQual=seq(min(transformed_data$OverallQual), max(transformed_data$OverallQual), length.out = 50),
  #OverallCond=seq(min(transformed_data$OverallCond), max(transformed_data$OverallCond), length.out = 50),
  #YearBuilt=seq(min(transformed_data$YearBuilt), max(transformed_data$YearBuilt), length.out = 50),
  #CentralAir=central_air,
  #X1stFlrSF=seq(min(transformed_data$X1stFlrSF), max(transformed_data$X1stFlrSF), length.out = 50)
  #X2ndFlrSF=seq(min(transformed_data$X2ndFlrSF), max(transformed_data$X2ndFlrSF), length.out = 50),
  #TotRmsAbvGrd=tot_rms_abv_grd,
  #TotRmsAbvGrd=seq(min(transformed_data$TotRmsAbvGrd), max(transformed_data$TotRmsAbvGrd), length.out = 50),
  #Fireplaces=fireplaces,
  #PavedDrive=paved_drive
#)

# Set K-fold parameters
knnControl <- trainControl(method="repeatedcv", number=10, repeats=1)
knn.fit <- train(SalePrice~., 
                 data=transformed_data, 
                 method="knn", 
                 trControl=knnControl, 
                 tuneGrid=expand.grid(k=c(1:10, 20, 30)))
plot(knn.fit)
knn.fit

# predictions <- predict(knn.fit, predictors)
# prediction.surface <- matrix(predictions, 301, 51)
# surface3d(0:300, 0:50, prediction.surface, alpha=0.4)
```

## Bagging with Multiple Linear Regression
```{r}
# Create a formula
formula <- SalePrice~.
# Set up train control for bagging
bootControl <- trainControl(method="boot", number=25)

# Perform Bagging
bagged_model <- bagging(formula, data = transformed_data, method="lm", trControl=bootControl, nbagg = 1)

# Predict and plot
predictions <- predict(bagging_model, transformed_data)
plot(X1stFlrSF, SalePrice, main = "Bagging with Linear Regression")
lines(X1stFlrSF, predictions, col = "red", lwd = 2)

```


## Bootstrapping and Bagging with Random Forest
```{r}
# Fit Random Forest
rf_model <- randomForest(SalePrice~., data = transformed_data, ntree = 500)
# Print model summary
print(rf_model)
# Plot error rate as a function of the number of trees
plot(rf_model)
# Get importance for each predictor
importance(rf_model)
varImpPlot(rf_model)

# Predict and plot
transformed_data$Predictions <- predict(rf_model, transformed_data)
# Confusion matrix
table(predictions, transformed_data$SalePrice)

# Transform data frame long 
transformed_data_long <- transformed_data %>%
    gather(key="Predictor", value="Value", -SalePrice, -Predictions)

ggplot(transformed_data_long, aes(x=Value, y=SalePrice)) +
  geom_point() +
  geom_line(aes(y=Predictions), color="blue") +
  scale_y_continuous(labels=comma) + # Adjust y-axis labeling
  facet_wrap(~Predictor, scales="free_x") + 
  labs(title="Sale Price vs. Predictors",
       x="Predictor Value",
       y="Sale Price ($ USD)") +
  theme_minimal()

# # Plot original data and predictions individually
# ggplot(transformed_data, aes(x = OverallCond, y = SalePrice)) +
#   geom_point() +
#   geom_line(aes(y = predictions), color = "blue", size = 1.5) +
#   ggtitle("Random Forest with Bootstrapping and Bagging") +
#   theme_minimal()
# 
# ggplot(transformed_data, aes(x = OverallQual, y = SalePrice)) +
#   geom_point() +
#   geom_line(aes(y = predictions), color = "blue", size = 1.5) +
#   ggtitle("Random Forest with Bootstrapping and Bagging") +
#   theme_minimal()
# 
# ggplot(transformed_data, aes(x = X1stFlrSF, y = SalePrice)) +
#   geom_point() +
#   geom_line(aes(y = predictions), color = "blue", size = 1.5) +
#   ggtitle("Random Forest with Bootstrapping and Bagging") +
#   theme_minimal()

```

## Ensemble multiple models
```{r}
# Define training control
ensembleControl <- trainControl(method = "cv", number = 5, savePredictions = "final")

# List of models to ensemble
model_list <- caretList(
  SalePrice~.,
  data = transformed_data,
  trControl = ensembleControl,
  methodList = c("lm", "glmnet", "rf")
)

# Stacking models using linear regression as meta-model
stackControl <- trainControl(method = "cv", number = 5)
stacked_model <- caretStack(
  model_list,
  method = "glm",
  trControl = stackControl
)

# Predict and plot
predictions <- predict(stacked_model, newdata = transformed_data)
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = predictions), color = "green", size = 1.5) +
  ggtitle("Ensemble (Stacking)") +
  theme_minimal()

```


