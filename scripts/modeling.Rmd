---
title: "model"
author: "ddxbugs"
date: "2024-05-27"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
sessionInfo()
getwd()

set.seed(1234)

if (!require(caret)) install.packages("caret")
if (!require(caret)) install.packages("lmboot")
if (!require(caret)) install.packages("glmnet")
if (!require(caret)) install.packages("olsrr")
if (!require(caret)) install.packages("car")
if (!require(caret)) install.packages("boot")
library(caret)
library(glmnet)
library(olsrr)
library(car)
library(lmboot)
library(boot)
```

## Import the data
```{r}
processed_data <- read.csv("../data/processed/processed_data.csv")
```

## Transform the data
```{r}
transformed_data <- processed_data %>%
  select(-X)
```

## Build a model
```{r}
fit <- lm(SalePrice~., data=transformed_data)
summary(fit)
anova(fit)
confint(fit)
plot(fit)
hist(fit$residuals, main="Histogram of Residuals", xlab="Residuals")
vif(fit)
barplot(vif(fit), main="VIF Values", horiz=TRUE, col="lightblue")
```

## Feature Selection
```{r}
ols_step_backward_p(fit, prem=0.05, details=FALSE)
ols_step_forward_p(fit, penter=0.05, details=FALSE)
ols_step_both_p(fit, prem=0.05, penter=0.05, details=TRUE)
```

## Stepwise selection MLR model
```{r}
stepwise_fit <- lm(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data)
summary(stepwise_fit)
anova(stepwise_fit)
confint(stepwise_fit)
plot(stepwise_fit)
hist(stepwise_fit$residuals, breaks="Sturges", main="Histogram of Residuals", xlab="Residuals")
vif(stepwise_fit)
barplot(vif(fit), main="VIF Values", horiz=TRUE, col="lightblue")
```

## OBJECTIVE 1
```{r}

# Bootstrapping 
# Bootstrap 95% CI
# Bagging with Linear Regression

train_control <- trainControl(method="cv", number=5)
model <- train(log(SalePrice)~., data=transformed_data, method="lm", trControl=train_control)
summary(model)

stepwise_model <- train(log(SalePrice) ~ OverallQual + X1stFlrSF + X2ndFlrSF + YearBuilt + OverallCond + Fireplaces + CentralAir + BldgType + Neighborhood + LandContour + TotRmsAbvGrd, data=transformed_data, method="lm", trControl=train_control)
summary(stepwise_model)

```

## OBJECTIVE 2

# Bootstrapping
```{r}
# Boot function calculates median Sale Price
median.func <- function(x, i) {
  median(x[i])
}
median.func(transformed_data$SalePrice, 1:30)
median(transformed_data$SalePrice)

median.boot <- boot(transformed_data$SalePrice, median.func, R=2000)
median.boot

# Obtain bootstrap statistical CI
boot.ci(median.boot, conf=0.95, type=c("perc", "basic", "bca"))

# Take a sample of 30 observations to estimate median of population
boot.sample <- sample(transformed_data$SalePrice, 30, replace=FALSE)
hist(boot.sample)
median(boot.sample)
# Compute the bootstrap sampling distribution of the median
medians <- c()
B <- 2000
for (i in 1:B) {
  b.sample <- sample(processed_data$SalePrice, 30, replace=TRUE)
  medians[i] <- median(b.sample)
}
# Plot sample Sale Price histogram distribution
hist(medians, main="Bootstrap Sampling Distribution", xlab="Median Sale Price ($ USD)")

# Obtain SE of median
sd(medians)
# 95% confidence interval
quantile(medians, probs=c(0.025,0.975))

boot.p <- paired.boot(log(SalePrice)~., data=transformed_data, B=2000, seed=1234)
# Obtain bootstrap percentile CI for each coefficient
apply(boot.p$bootEstParam, 2, quantile, probs=c(0.025, 0.975))

boot.res <- residual.boot(log(SalePrice)~., data=transformed_data, B=2000, seed=1234)
apply(boot.res$bootEstParam, 2, quantile, probs=c(0.025, 0.975))
```

## Compare Bootstrapping vs. T-test result 
```{r}
t.result <- t.test(transformed_data$SalePrice)
xbar <- c()
B <- 2000
for (i in 1:B) {
  b.sample <- sample(transformed_data$SalePrice, 30, replace=TRUE)
  xbar[i] <- mean(b.sample)
}
# Plot xbar Sale Price histogram distribution
hist(xbar, main="Bootstrap Sampling Distribution", xlab="Average Sale Price")
# Obtain SE of the xbar
sd(xbar)
# 95% Confidence interval for population mean
quantile(xbar, probs=c(0.025, 0.975))
t.result$conf.int
# T-test SE of mean
sd(transformed_data$SalePrice)/sqrt(30)
# Bootstrap SE for the mean
sd(xbar)
```


```{r}
knnControl <- trainControl(method="repeatedcv", number=10, repeats=1)
knn.fit <- train(SalePrice~., data=processed_data, method="knn", trControl=knnControl)
plot(knn.fit)
knn.fit
```
```{r}
pred <- predict(knn.fit, predictors)
pred.surface <- matrix(preds, 301, 51)
plot3d()
surface3d()
```



```{r}
# Random Forest Bootstrapping

# Random Forest Bagging
library(randomForest)
library(ggplot2)

# Set the simulated data seed
set.seed(1234)

```

## Bagging with Multiple Linear Regression
```{r}
# Load necessary libraries
library(caret)
library(ipred)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Create a formula
formula <- y ~ poly(x, 4, raw=TRUE)

# Perform Bagging
set.seed(123)
bagging_model <- bagging(formula, data = data, nbagg = 50)

# Predict and plot
preds <- predict(bagging_model, data)
plot(x, y, main = "Bagging with Linear Regression")
lines(x, preds, col = "red", lwd = 2)

```


## Bootstrapping and Bagging with Random Forest
```{r}
# Load necessary libraries
library(randomForest)
library(ggplot2)

# Fit Random Forest
set.seed(123)
rf_model <- randomForest(SalePrice ~ ., data = processed_data, ntree = 500)

# Predict and plot
preds <- predict(rf_model, data)

# Plot original data and predictions
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = preds), color = "blue", size = 1.5) +
  ggtitle("Random Forest with Bootstrapping and Bagging") +
  theme_minimal()

```


## Bootstrapping with Random Forest Manually
```{r}
# Load necessary libraries
library(caret)
library(rpart)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Number of bootstraps
num_bootstrap <- 50

# Store predictions
pred_matrix <- matrix(NA, nrow = num_bootstrap, ncol = length(y))

# Perform bootstrapping
set.seed(123)
for (i in 1:num_bootstrap) {
  # Create bootstrap sample
  boot_indices <- sample(1:nrow(data), replace = TRUE)
  boot_data <- data[boot_indices, ]
  
  # Train decision tree on bootstrap sample
  tree_model <- rpart(y ~ x, data = boot_data)
  
  # Predict on original data
  pred_matrix[i, ] <- predict(tree_model, newdata = data)
}

# Aggregate predictions (bagging)
bagged_preds <- apply(pred_matrix, 2, mean)

# Plot original data and bagged predictions
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = bagged_preds), color = "red", size = 1.5) +
  ggtitle("Bagging with Decision Trees") +
  theme_minimal()
```


## Ensemble multiple models
```{r}
# Load necessary libraries
library(caret)
library(glmnet)
library(randomForest)

# Simulate some data
set.seed(123)
x <- runif(100, -6, 6.5)
y <- 2 * x^4 - 2 * x^3 - 50 * x^2 + 100 * x + 2 + rnorm(100, 0, 100)
data <- data.frame(x = x, y = y)

# Define training control
train_control <- trainControl(method = "cv", number = 5, savePredictions = "final")

# List of models to ensemble
model_list <- caretList(
  y ~ poly(x, 4, raw=TRUE),
  data = data,
  trControl = train_control,
  methodList = c("lm", "glmnet", "rf")
)

# Stacking models using linear regression as meta-model
stack_control <- trainControl(method = "cv", number = 5)
stacked_model <- caretStack(
  model_list,
  method = "glm",
  trControl = stack_control
)

# Predict and plot
preds <- predict(stacked_model, newdata = data)
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = preds), color = "green", size = 1.5) +
  ggtitle("Ensemble (Stacking)") +
  theme_minimal()

```


